


# !pip install torch torchvision


!python --version


# imports 
import os
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from sklearn.metrics import f1_score
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import random


# define the device to use
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using {device} device")


torch.manual_seed(10)





# define directory
data_dir = "./data"

# transform to convert input into pytorch tensor
transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])

# train and test data 
train_dataset = datasets.ImageFolder(root = os.path.join(data_dir, "train"), transform = transform)
test_dataset  = datasets.ImageFolder(root = os.path.join(data_dir, "test"), transform = transform)

# data loader objects for efficient batch processing during training
train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)
test_loader  = DataLoader(test_dataset, batch_size = 32, shuffle = False)


X_train = []
y_train = []
for X, y in train_loader:
    X_train.append(X)
    y_train.append(y)
# concatenate into one tensor
X_train = torch.cat(X_train, dim = 0)
y_train = torch.cat(y_train, dim = 0)

X_test = []
y_test = []
for X, y in test_loader:
    X_test.append(X)
    y_test.append(y)
# concatenate into one tensor
X_test = torch.cat(X_test, dim = 0)
y_test = torch.cat(y_test, dim = 0)


# define input dims and num classes, global variables beacuse they dont change
input_dim = 224 * 224 * 3
num_classes = len(train_dataset.classes)
# criterion stays the same (cross entropy) because its multiclass classification
criterion = nn.CrossEntropyLoss()





def train_model(NN, optimizer, criterion, X_train, y_train, X_test, y_test, num_epochs = 100):

    model = NN().to(device)
    
    train_losses = []
    test_losses = []
    
    for epoch in range(num_epochs):
        # training
        model.train()
        
        # forward
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
    
        # backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_losses.append(loss.item())
    
        # testing
        model.eval()
        with torch.no_grad():
            test_outputs = model(X_test)
            test_loss = criterion(test_outputs, y_test).item()
            test_losses.append(test_loss)
    
        if (epoch+1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Validation Loss: {test_loss:.4f}")

    return model, train_losses, test_losses


def plot_losses(train_losses, test_losses):
    plt.figure(figsize = (8,5))
    plt.plot(train_losses, label = "Training loss")
    plt.plot(test_losses, label = "Testing loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("Training vs testing loss")
    plt.legend()
    plt.grid()
    plt.show()


def get_f1(model, X_train, y_train, X_test, y_test):
    with torch.no_grad():
        train_preds = model(X_train).argmax(dim = 1).cpu().numpy()
        train_labels = y_train.cpu().numpy()
        train_f1 = f1_score(train_labels, train_preds, average = "macro")
    
        test_preds = model(X_test).argmax(dim = 1).cpu().numpy()
        test_labels = y_test.cpu().numpy()
        test_f1 = f1_score(test_labels, test_preds, average = "macro")
    
    print(f"Train F1: {train_f1:.4f} \nTest F1: {test_f1:.4f}")
    return train_f1, test_f1





class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        # flatten input
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, num_classes),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

# 
model_1 = NeuralNetwork().to(device)
print(model_1)





# define optimizer
optimizer = torch.optim.Adam(model_1.parameters(), lr=0.001)

# training the model
model, train_losses, test_losses = train_model(NeuralNetwork, optimizer, criterion, X_train, y_train, X_test, y_test, num_epochs = 100)


# plotting losses
plot_losses(train_losses, test_losses)


# get train and test f1 scores
get_f1(model, X_train, y_train, X_test, y_test)








class ComplexNeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        # flatten input
        self.flatten = nn.Flatten()

        self.linear_relu_stack = nn.Sequential(
            nn.Linear(input_dim, 1024),
            # nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Dropout(0.4),

            nn.Linear(1024, 1024),
            # nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Dropout(0.4),

            nn.Linear(1024, 512),
            # nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(512, 512),
            # nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(512, 256),
            # nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.2),

            # Output layer
            nn.Linear(256, num_classes),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

# 
model_2 = ComplexNeuralNetwork().to(device)
print(model_2)


# define optimizer
optimizer = torch.optim.Adam(model_1.parameters(), lr=0.001)

# trainning model
model, train_losses, test_losses = train_model(ComplexNeuralNetwork, optimizer, criterion, X_train, y_train, X_test, y_test, num_epochs = 100)


# plotting losses
plot_losses(train_losses, test_losses)


# get train and test f1 scores
get_f1(model, X_train, y_train, X_test, y_test)





# create a validation split from the training data  because we will be training multiple models
X_tr, X_val, y_tr, y_val = train_test_split(
    X_train, y_train,
    test_size = 0.2,
    random_state = 42,
    stratify = y_train
)


learning_rates = [0.0005, 0.001, 0.002, 0.0025, 0.005, 0.0075, 0.01, 0.015, 0.02, 0.025, 0.05, 0.075, 0.09]
random.shuffle(learning_rates)
best_params = {"train_f1:": 0, "val_f1": 0, "test_f1": 0, "learning_rate": None, "model_state": None}
for lr_ in learning_rates:
    print(f"\nTraining with learning rate = {lr_}")
    
    optimizer = torch.optim.Adam(model_3.parameters(), lr = lr_)
    # trainning model
    model, train_losses, val_losses = train_model(model_2, optimizer, criterion, X_tr, y_tr, X_val, y_val, num_epochs = 100)

    # plotting losses
    plot_losses(train_losses, test_losses)

    # get train and test f1 scores
    train_f1, val_f1 = get_f1(model, X_tr, y_tr, X_val, y_val)
    
    if val_f1 > best_params["val_f1"]:
        # evaluate this model on full train and test sets
        train_f1, test_f1 = get_f1(model_lr, X_train, y_train, X_test, y_test)

        best_params["val_f1"] = val_f1
        best_params["train_f1"] = train_f1
        best_params["test_f1"] = test_f1
        best_params["learning_rate"] = lr_
        best_params["model_state"] = copy.deepcopy(model_lr.state_dict())
print("---------------------------------------------\nDone training")
print("Best parameters:")
print(f'\tTrain F1: {best_params["train_f1"]: .4f} \n\tTest F1: {best_params["test f1"]: .4f} \n\tLearning rate: {best_params["learning rate"]}')


print("Best parameters:")
print(f'\tTrain F1: {best_params["train_f1"]: .4f} \n\tTest F1: {best_params["test f1"]: .4f} \n\tLearning rate: {best_params["learning rate"]}')


# rebuild model based on best output

best_model = ComplexNeuralNetwork().to(device)
best_model.load_state_dict(best_params["model_state"])





optimizer = torch.optim.Adam(model_3.parameters(), lr = 0.0025)
run_and_plot(model_3, optimizer, criterion, num_epochs = 100)
train_f1, test_f1 = get_f1(model_3)






